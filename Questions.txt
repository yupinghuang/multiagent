1) Question 1 in the project provides an example where minimax predicts Pacman will do quite poorly, but he actually often wins the game. Briefly explain why this might occur. (There are a number of possible responses to this question; you need only give one.)

Firstly because the ghosts don¡¯t appear to be optimal players.

Secondly because the maze is small with dots within 3-4 steps to the player. 


2) Describe the evaluation function in Question 4 - what features does it include, how do you combine them. This can be copied/pasted from your comments in the code; it's mainly here to make sure that somewhere, you describe your function! 

The evaluation function here deals with 2 situations: when ghost is close (within 3 steps) or far
1.when ghost is far away, the evaluation doesn't care about where the ghost is but only evaluates the closest food
2.when ghost is close, the evaluation considers both the ghost distance and also the closest food.
(subtracting the distances together from terminal utility)


3) MCTS: Run your algorithm for 15 games with rollouts set to 75. This may take a little time, but not more than a minute or two. How many games does player 1 win? 

Player 1 won 15/15.

4) Repeat (4), except now make MCTS player 2. How many games does player 1 win? (If your answers to (4) and (5) don't make sense to you based on which player should be the smarter player, you probably have an error in your implementation.)

Player 1 won 0/15.

5) Experiment with several values of the UCB exploration constant. Report what values you used and your results for how this affects your MCTS agent's effectiveness. Explain your hypotheses about why you see these results, tying back to what you know about how varying this constant affects the algorithm. Your experimentation should be sufficiently extensive to explore at least some trend in the results.

We used the following UCB constant values with rollouts set to 75 for 60 games:

rollouts 20, 500 games
0.1	445
0.2	440
0.5	449
0.8	457
1.0	439
5.0	451
10.	435
100.	437
1000.	446


6) Change the UCB exploration constant back to its original value (.5) and experiment with two MCTS players with different numbers of rollouts relative to one another. For example, you might look at an agent with 10 rollouts and an agent with 20 rollouts, and then look at an agent with 20 rollouts versus an agent with 40 rollouts. As in (6) report what values you used and your results. Explain your hypotheses about why you see these results, tying back to what you know about the algorithm. Your experimentation should be sufficiently extensive to explore at least some trend in the results.
(100 games)
Here¡¯s our player 1 win counts for 100 games
10v10, 64
10v20, 47
10v30, 36
10v40, 20
20v20, 60 (2 draws)
30v30, 59
30v35, 52
30v37, 46 (3 draws)
100v100, 58 (2 draw)

We know that in the algorithm, the more rollouts we have, the closer the value of each
node is to the value if a complete tree search is done. Also from Wikipedia it seems like in this game, the first player has a slight advantage. In our data, we can see that if
we fill the rollouts for player 1, the higher the rollouts for player 2, the less likely
it is for player 1 to win. If both players have the same rollouts, then player 1 is
slightly more likely to win. It seems like we can force a 50-50 chance if the rollouts of
player 2 is slightly higher than the rollouts of player 1.


7) Optional extension: If you did the optional extension at the end of the project, say so here and describe what your agent does.